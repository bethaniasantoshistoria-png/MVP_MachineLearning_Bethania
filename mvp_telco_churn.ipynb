{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b1ccab",
   "metadata": {},
   "source": [
    "\n",
    "# MVP — Machine Learning\n",
    "### Disciplina: Ciência de Dados e Analytics\n",
    "### Aluna: Bethânia Alves\n",
    "\n",
    "---\n"
   ]
  },
  {

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9824aae1",
   "metadata": {},
   "source": [
    "\n",
    "# MVP — Classificação de Churn (Telco Customer Churn)\n",
    "**Tipo de problema:** Classificação binária  \n",
    "**Domínio:** Dados tabulares (negócios/telecom)  \n",
    "**Execução:** Google Colab (instala dependências automaticamente)  \n",
    "**Dataset (URL pública):** \n",
    "- `https://raw.githubusercontent.com/blastchar/telco-customer-churn/master/WA_Fn-UseC_-Telco-Customer-Churn.csv`\n",
    "\n",
    "> **TL;DR**: Construímos um pipeline de ML para prever `Churn` usando **baseline**, **Logistic Regression**, **Random Forest** e **XGBoost**, com validação estratificada, **tuning** de hiperparâmetros, métricas (ROC-AUC, PR-AUC, F1, Recall, MCC) e análise de importância de atributos (permutações). O notebook segue boas práticas (seeds, pipelines, sem *data leakage*) e pode ser executado de ponta a ponta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82da0b6",
   "metadata": {},
   "source": [
    "## 1. Setup: dependências, imports, seeds, versões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Se estiver no Colab, descomente se precisar de versões específicas:\n",
    "# !pip -q install xgboost==2.0.3\n",
    "\n",
    "import os, sys, random, time, platform, math, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelagem\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, f1_score, recall_score, precision_score, \n",
    "                             confusion_matrix, classification_report, matthews_corrcoef)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# XGBoost (opcional, mas incluído)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except Exception as e:\n",
    "    print(\"XGBoost não disponível. Rode '!pip install xgboost' no Colab se desejar usar.\")\n",
    "    xgb_available = False\n",
    "\n",
    "# Seeds para reprodutibilidade\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print({\n",
    "    'python': platform.python_version(),\n",
    "    'numpy': np.__version__,\n",
    "    'pandas': pd.__version__\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd4565",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Definição do Problema e Dataset\n",
    "**Objetivo:** Prever se um cliente irá cancelar (`Churn`) com base em atributos contratuais, demográficos e de serviço.  \n",
    "**Métrica-alvo de negócio (exemplo):** Maximizar **Recall** para reduzir *false negatives* (não perder churners), mantendo **Precision** razoável; acompanhamos **ROC-AUC** e **PR-AUC** para visão global.\n",
    "\n",
    "**Premissas/Hipóteses:**  \n",
    "- Há **desbalanceamento** de classes (tipicamente `Churn` é minoria).  \n",
    "- Algumas variáveis numéricas podem estar como texto (ex.: `TotalCharges`).  \n",
    "- A relação entre atributos e *target* pode ser não linear; por isso comparamos modelos lineares e baseados em árvores.\n",
    "\n",
    "**Restrições & Ética:**  \n",
    "- Dataset público e **não confidencial**.  \n",
    "- Evitar *data leakage* cuidando da ordem *fit/transform* com pipelines e splits.  \n",
    "- Atenção a vieses: atributos demográficos podem introduzir discriminação; interpretar resultados com responsabilidade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c786627",
   "metadata": {},
   "source": [
    "### 2.1 Leitura dos dados por URL pública (GitHub Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e00239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URL = \"https://raw.githubusercontent.com/blastchar/telco-customer-churn/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(URL)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805011f",
   "metadata": {},
   "source": [
    "## 3. EDA: qualidade e distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afeb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df.sample(5, random_state=SEED))\n",
    "print(\"\\nTipos:\\n\", df.dtypes.value_counts())\n",
    "\n",
    "# Missing values\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing (top 10):\\n\", missing.head(10))\n",
    "\n",
    "# Target balance\n",
    "print(\"\\nBalanceamento de 'Churn':\\n\", df['Churn'].value_counts(normalize=True))\n",
    "\n",
    "# Converter TotalCharges para numérico (muitos notebooks esquecem)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "print(\"\\nTotalCharges convertida para numérico. Missing após conversão:\", df['TotalCharges'].isna().sum())\n",
    "\n",
    "# Estatísticas descritivas\n",
    "display(df.describe(include='all').T.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460fbe3",
   "metadata": {},
   "source": [
    "## 4. Preparação: split estratificado + definição de features/target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fa33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = 'Churn'\n",
    "y = (df[TARGET] == 'Yes').astype(int)  # binário 1=churn\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "# Remover identificadores diretos\n",
    "if 'customerID' in X.columns:\n",
    "    X = X.drop(columns=['customerID'])\n",
    "\n",
    "# Separar tipos\n",
    "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(\"Numéricas:\", num_cols)\n",
    "print(\"Categóricas:\", cat_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "print(X_train.shape, X_test.shape, y_train.mean(), y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601c126",
   "metadata": {},
   "source": [
    "## 5. Pipelines: prevenção de vazamento e transformações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_tf = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_tf, num_cols),\n",
    "    (\"cat\", categorical_tf, cat_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104ee82",
   "metadata": {},
   "source": [
    "## 6. Modelagem: baseline e modelos candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bce7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"DummyMostFrequent\": DummyClassifier(strategy=\"most_frequent\", random_state=SEED),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, n_jobs=None, random_state=SEED),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, n_jobs=-1, random_state=SEED, class_weight=\"balanced_subsample\"\n",
    "    )\n",
    "}\n",
    "\n",
    "if xgb_available:\n",
    "    models[\"XGBoost\"] = XGBClassifier(\n",
    "        n_estimators=400, max_depth=4, learning_rate=0.1, subsample=0.9, colsample_bytree=0.9,\n",
    "        eval_metric=\"logloss\", random_state=SEED, n_jobs=-1, reg_lambda=1.0, tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'pr_auc': 'average_precision',\n",
    "    'f1': 'f1',\n",
    "    'recall': 'recall',\n",
    "    'precision': 'precision'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, est in models.items():\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", est)])\n",
    "    cv_res = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    results[name] = {k: (v.mean(), v.std()) for k, v in cv_res.items() if k.startswith(\"test_\")}\n",
    "\n",
    "pd.DataFrame({m: {k: f\"{v[0]:.3f} ± {v[1]:.3f}\" for k, v in met.items()} for m, met in results.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450873b",
   "metadata": {},
   "source": [
    "## 7. Otimização de hiperparâmetros (RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuned_models = {}\n",
    "\n",
    "# Random Forest space\n",
    "rf_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", RandomForestClassifier(random_state=SEED, n_jobs=-1))])\n",
    "rf_space = {\n",
    "    \"model__n_estimators\": [200, 300, 500, 800],\n",
    "    \"model__max_depth\": [None, 6, 8, 10, 12],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipe, rf_space, n_iter=20, cv=cv, scoring=\"average_precision\", n_jobs=-1, random_state=SEED, verbose=1\n",
    ")\n",
    "rf_search.fit(X_train, y_train)\n",
    "tuned_models[\"RandomForest\"] = rf_search.best_estimator_\n",
    "print(\"RF best:\", rf_search.best_params_)\n",
    "\n",
    "if xgb_available:\n",
    "    xgb_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", XGBClassifier(\n",
    "        random_state=SEED, n_jobs=-1, eval_metric=\"logloss\", tree_method=\"hist\"\n",
    "    ))])\n",
    "    xgb_space = {\n",
    "        \"model__n_estimators\": [200, 400, 600, 800],\n",
    "        \"model__max_depth\": [3, 4, 5, 6],\n",
    "        \"model__learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "        \"model__subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"model__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "        \"model__reg_lambda\": [0.0, 0.5, 1.0, 2.0]\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xgb_pipe, xgb_space, n_iter=24, cv=cv, scoring=\"average_precision\", n_jobs=-1, random_state=SEED, verbose=1\n",
    "    )\n",
    "    xgb_search.fit(X_train, y_train)\n",
    "    tuned_models[\"XGBoost\"] = xgb_search.best_estimator_\n",
    "    print(\"XGB best:\", xgb_search.best_params_)\n",
    "\n",
    "tuned_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ad61e",
   "metadata": {},
   "source": [
    "## 8. Avaliação final no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a593084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X_tr, y_tr, X_te, y_te, name=\"Model\"):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    proba = model.predict_proba(X_te)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    pred = model.predict(X_te)\n",
    "    \n",
    "    roc = roc_auc_score(y_te, proba) if proba is not None else np.nan\n",
    "    pr = average_precision_score(y_te, proba) if proba is not None else np.nan\n",
    "    f1 = f1_score(y_te, pred)\n",
    "    rec = recall_score(y_te, pred)\n",
    "    pre = precision_score(y_te, pred)\n",
    "    mcc = matthews_corrcoef(y_te, pred)\n",
    "    cm = confusion_matrix(y_te, pred)\n",
    "    \n",
    "    print(f\"\\n== {name} ==\")\n",
    "    print(f\"ROC-AUC: {roc:.3f} | PR-AUC: {pr:.3f} | F1: {f1:.3f} | Recall: {rec:.3f} | Precision: {pre:.3f} | MCC: {mcc:.3f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_te, pred, digits=3))\n",
    "    \n",
    "    return {\"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"recall\": rec, \"precision\": pre, \"mcc\": mcc, \"cm\": cm}\n",
    "\n",
    "# Escolher melhor entre os tunados (e um baseline forte para comparação)\n",
    "final_scores = {}\n",
    "# Logistic sem tuning explícito para comparação\n",
    "log_pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", LogisticRegression(max_iter=200, random_state=SEED))])\n",
    "final_scores[\"LogReg\"] = evaluate(log_pipe, X_train, y_train, X_test, y_test, \"LogisticRegression\")\n",
    "\n",
    "# Tunados\n",
    "for mname, mest in tuned_models.items():\n",
    "    final_scores[mname] = evaluate(mest, X_train, y_train, X_test, y_test, mname)\n",
    "\n",
    "final_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66d229",
   "metadata": {},
   "source": [
    "## 9. Interpretação — Importância por Permutação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleciona o melhor modelo por PR-AUC (priorizando classe positiva rara)\n",
    "best_model_name = max(final_scores, key=lambda k: (0 if np.isnan(final_scores[k]['pr_auc']) else final_scores[k]['pr_auc']))\n",
    "print(\"Melhor por PR-AUC:\", best_model_name)\n",
    "best_model = tuned_models.get(best_model_name, log_pipe)\n",
    "\n",
    "# Ajusta no treino completo\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Para recuperar nomes das features após OHE\n",
    "# Atenção: para ColumnTransformer, precisamos obter as colunas transformadas\n",
    "ohe = best_model.named_steps['prep'].named_transformers_['cat'].named_steps['ohe']\n",
    "num_features = num_cols\n",
    "cat_features = list(ohe.get_feature_names_out(cat_cols))\n",
    "all_features = num_features + cat_features\n",
    "\n",
    "# Importância por permutação no teste\n",
    "perm = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=SEED, n_jobs=-1, scoring=\"average_precision\"\n",
    ")\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "imp_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f4d4e",
   "metadata": {},
   "source": [
    "## 10. Visualizações rápidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b07d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# Matriz de confusão\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['No', 'Yes'], rotation=45)\n",
    "plt.yticks(tick_marks, ['No', 'Yes'])\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Curvas ROC e PR\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_score = best_model.predict_proba(X_test)[:, 1]\n",
    "    RocCurveDisplay.from_predictions(y_test, y_score)\n",
    "    plt.show()\n",
    "    PrecisionRecallDisplay.from_predictions(y_test, y_score)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Modelo não possui predict_proba para curvas ROC/PR.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a37b84",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Conclusões e Próximos Passos\n",
    "**Achados principais:**  \n",
    "- O pipeline reprodutível com *preprocessing* (imputação + OHE + *scaling*) evitou vazamentos.  \n",
    "- Entre os modelos testados, o melhor por **PR-AUC** tende a ser baseado em árvores (RF/XGBoost), equilibrando **Recall** e **Precision**.  \n",
    "- **Importância por permutação** sugere que atributos como `tenure`, `MonthlyCharges`, `Contract`, `InternetService` e `PaymentMethod` influenciam a probabilidade de churn.\n",
    "\n",
    "**Limitações & melhorias:**  \n",
    "- Desbalanceamento ainda pode afetar *thresholds*; considerar calibrar probabilidade e/ou otimizar limites específicos por custo.  \n",
    "- Avaliar **fairness** e impacto de variáveis potencialmente sensíveis.  \n",
    "- Testar **ensembles** (stacking) e outras técnicas de regularização/seleção de variáveis.  \n",
    "- Incluir rastreamento de experimentos (ex.: MLflow) e *backtesting* de *drift* com dados temporais reais.\n",
    "\n",
    "**Recursos computacionais:** executado em CPU; tempo de tuning depende do Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b84a7b",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Checklist Respondido\n",
    "**Definição do Problema**  \n",
    "- Descrição do problema: prever cancelamento (churn) de clientes.  \n",
    "- Premissas: desbalanceamento; necessidade de evitar vazamento; não linearidades.  \n",
    "- Restrições: uso público; ética/fairness.  \n",
    "- Dataset: tabular, ~7k linhas; misto cat/num; `Churn` como rótulo.\n",
    "\n",
    "**Preparação de Dados**  \n",
    "- Split: treino/teste estratificado (80/20) + *CV* 5-fold estratificado.  \n",
    "- Validação cruzada: sim, para avaliar generalização.  \n",
    "- Transformações: imputação, OHE, *scaling* em pipeline.  \n",
    "- Feature engineering/selection: implícita via modelos e análise de importância.\n",
    "\n",
    "**Modelagem e Treinamento**  \n",
    "- Algoritmos: Dummy (baseline), Logistic Regression, Random Forest, XGBoost.  \n",
    "- Hiperparâmetros: *RandomizedSearchCV* para RF/XGBoost.  \n",
    "- Under/overfitting: monitorado via CV e curvas; árvores podem sobreajustar sem tuning adequado.  \n",
    "- Ensamble: RF/XGB já são ensambles; stacking é próximo passo.\n",
    "\n",
    "**Avaliação de Resultados**  \n",
    "- Métricas: ROC-AUC, PR-AUC, F1, Recall, Precision, MCC; matriz de confusão e curvas ROC/PR.  \n",
    "- Melhor solução: escolhida por PR-AUC no teste; justificar trocas entre Recall/Precision conforme custo do negócio.\n",
    "\n",
    "**Reprodutibilidade & Boas Práticas**  \n",
    "- Seeds fixas; pipelines; decisões documentadas; URL pública utilizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc92844",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Apêndice — Versões e Seeds\n",
    "- `SEED = 42`  \n",
    "- Ver versões impressas no **Setup**.  \n",
    "- Para usar GPU/TPU no Colab, configure em `Ambiente de execução > Alterar tipo de ambiente de execução`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
